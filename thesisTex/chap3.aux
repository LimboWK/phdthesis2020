\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{pdg}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}$K_S^0$ Reconstruction Study}{49}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Cut-based $K_S^0$ Reconstruction}{50}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-1}{\ignorespaces The left is transverse flight length and the right is the total flight length from true $K_S^0$. The blue is from generic MC and the orange is from signal MC. It's clear that the average $K_S^0$ flight length in signal MC is longer. Both plots are normalized.\relax }}{50}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ks_flight}{{3-1}{50}{The left is transverse flight length and the right is the total flight length from true $K_S^0$. The blue is from generic MC and the orange is from signal MC. It's clear that the average $K_S^0$ flight length in signal MC is longer. Both plots are normalized.\relax }{figure.caption.23}{}}
\citation{krohn2020global}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Pre-selection criteria of $\pi ^+ \pi ^-$ for $K_S^0$ offline reconstruction.\relax }}{51}{table.caption.24}\protected@file@percent }
\newlabel{tab:kspipi_select}{{3.1}{51}{Pre-selection criteria of $\pi ^+ \pi ^-$ for $K_S^0$ offline reconstruction.\relax }{table.caption.24}{}}
\citation{b2book}
\citation{feindt2006neurobayes}
\@writefile{lof}{\contentsline {figure}{\numberline {3-2}{\ignorespaces ``M" of $K_S^0$ from cut-based selection in signal MC (normalized). The blue line is the true $K_S^0$ and the orange is the fake $K_S^0$. \relax }}{52}{figure.caption.25}\protected@file@percent }
\newlabel{fig:ksM_sigmc}{{3-2}{52}{``M" of $K_S^0$ from cut-based selection in signal MC (normalized). The blue line is the true $K_S^0$ and the orange is the fake $K_S^0$. \relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}MVA-based $K_S^0$ Identification: KsFinder}{52}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Experience from Belle}{52}{subsection.3.2.1}\protected@file@percent }
\citation{kang2020measurement}
\citation{kang2020measurement}
\citation{keck2016fastbdt}
\citation{keck2016fastbdt}
\@writefile{lof}{\contentsline {figure}{\numberline {3-3}{\ignorespaces The distribution of two variables output from ``nisKsFinder": ``nb\_nolam" and ``nb\_vlike" for $K_S^0$ candidates from Belle signal MC. The left is from true $K_S^0$ and the right is from the fake $K_S^0$. In Belle, the standard cuts for $K_S^0$ is nb\_vlike $> 0.5$ and nb\_nolam $> -0.4$\cite  {kang2020measurement}.\relax }}{54}{figure.caption.26}\protected@file@percent }
\newlabel{b1niskf}{{3-3}{54}{The distribution of two variables output from ``nisKsFinder": ``nb\_nolam" and ``nb\_vlike" for $K_S^0$ candidates from Belle signal MC. The left is from true $K_S^0$ and the right is from the fake $K_S^0$. In Belle, the standard cuts for $K_S^0$ is nb\_vlike $> 0.5$ and nb\_nolam $> -0.4$\cite {kang2020measurement}.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}FastBDT algorithm}{54}{subsection.3.2.2}\protected@file@percent }
\citation{olshen1984classification}
\citation{friedman2002stochastic}
\@writefile{lof}{\contentsline {figure}{\numberline {3-4}{\ignorespaces Basic structure of a DT with depth = 3 and labels (features) of x,y,z. The terminal layer contains training data points and are separated by cuts on layer 1 to 3. The number (color demonstrated) is the signal fraction of training data points in each terminal node, which is used for testing data points signal probability.\relax }}{55}{figure.caption.27}\protected@file@percent }
\newlabel{fig:DT}{{3-4}{55}{Basic structure of a DT with depth = 3 and labels (features) of x,y,z. The terminal layer contains training data points and are separated by cuts on layer 1 to 3. The number (color demonstrated) is the signal fraction of training data points in each terminal node, which is used for testing data points signal probability.\relax }{figure.caption.27}{}}
\citation{friedman2001greedy}
\citation{keck2016fastbdt}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Decay Topology of $K_S^0 \to \pi ^+ \pi ^-$}{56}{subsection.3.2.3}\protected@file@percent }
\newlabel{fig:side:a}{{\caption@xref {fig:side:a}{ on input line 118}}{57}{Decay Topology of $K_S^0 \to \pi ^+ \pi ^-$}{figure.caption.28}{}}
\newlabel{fig:side:b}{{\caption@xref {fig:side:b}{ on input line 124}}{57}{Decay Topology of $K_S^0 \to \pi ^+ \pi ^-$}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3-5}{\ignorespaces The left shows the case when a charged track (not a pion) combined with a true pion as a fake $K_S^0$, the right shows the case when two daughters are correctly reconstructed as pion but not from the correct mother. \relax }}{57}{figure.caption.28}\protected@file@percent }
\newlabel{fig:side:a}{{\caption@xref {fig:side:a}{ on input line 138}}{58}{Decay Topology of $K_S^0 \to \pi ^+ \pi ^-$}{figure.caption.29}{}}
\newlabel{fig:side:b}{{\caption@xref {fig:side:b}{ on input line 144}}{58}{Decay Topology of $K_S^0 \to \pi ^+ \pi ^-$}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3-6}{\ignorespaces The left shows the $\Lambda \to p^+ \pi ^-$ decay shape that can be treated as $K_S^0$, the right shows a self-loop formed by a low $p_T$ charged pion reconstructed as two separated tracks with a vertex\relax }}{58}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Determination of training observables from $K_S^0$ decay }{58}{subsection.3.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-7}{\ignorespaces The decay shape parameters, vertex vector takes IP as origin. \relax }}{59}{figure.caption.30}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-8}{\ignorespaces The correlation matrix of the chosen observables. It shows different correlation between observables in signal and background. And there's no single observable showing strong correlation with all other members. \relax }}{60}{figure.caption.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Training, Testing and Application of KsFinder}{61}{subsection.3.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}The Performance and Over-fitting check}{61}{subsection.3.2.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces The Abbreviations.\relax }}{62}{table.caption.32}\protected@file@percent }
\newlabel{fig:side:a}{{3.2}{62}{The Abbreviations.\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Importance rank \relax }}{62}{table.caption.32}\protected@file@percent }
\newlabel{fig:side:b}{{3.3}{62}{Importance rank \relax }{table.caption.32}{}}
\newlabel{fig:side:a}{{\caption@xref {fig:side:a}{ on input line 268}}{63}{The Performance and Over-fitting check}{figure.caption.33}{}}
\newlabel{fig:side:b}{{\caption@xref {fig:side:b}{ on input line 273}}{63}{The Performance and Over-fitting check}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3-9}{\ignorespaces The left is ROC curve(blue for training and orange for testing) and the right is efficiency and purity (blue for efficiency and orange for purity) depending on cut of classifier output. Results are from $B^0 \to K_S^0 K_S^0 K_S^0$ sample.\relax }}{63}{figure.caption.33}\protected@file@percent }
\newlabel{fig:side:a}{{\caption@xref {fig:side:a}{ on input line 282}}{63}{The Performance and Over-fitting check}{figure.caption.34}{}}
\newlabel{fig:side:b}{{\caption@xref {fig:side:b}{ on input line 287}}{63}{The Performance and Over-fitting check}{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3-10}{\ignorespaces The left is ROC curve (blue for training and orange for testing) and the right is efficiency and purity (blue for efficiency and orange for purity) depending on cut of classifier output. Results are from $B^0$ generic decay sample.\relax }}{63}{figure.caption.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-12}{\ignorespaces FOM of classifier output (FBDT\_Ks) in $B^0 \to K_S^0 K_S^0 K_S^0$, maximum value is obtained at 0.74 and curve is almost flat after 0.5.\relax }}{65}{figure.caption.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.7}Data Validation for Classifier}{65}{subsection.3.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-13}{\ignorespaces $K_S^0$ purity improvement with cut value of FBDT\_Ks at 0.74 applied. Blue solid line is true $K_S^0$ before KsFinder and green dashed line is the true $K_S^0$ after. The orange solid line is fake $K_S^0$ before KsFinder and red dashed line is fake $K_S^0$ after. 95.3\% of true $K_S^0$ are kept while 97.6\% the fake are rejected by the classification. \relax }}{66}{figure.caption.37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-14}{\ignorespaces The distribution of invariant mass from daughters and the momentum of x,y,z direction. Blue bar is from all MC13, cyan bar is the true $K_S^0$ in it. Yellow step histogram is data with no cut, solid red data with MC13 trained cut, and the dashed is with MC12b cut. Experimental data has a good agreement with MC before and after applying the KsFinder.\relax }}{67}{figure.caption.38}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.8}KsFinder Effects on Kinematics Evaluation}{67}{subsection.3.2.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-15}{\ignorespaces Invariant mass fit of $K_S^0$ using cut at 0.2(loose) and 0.9(tight) to calculate $S_{data/MC}$.\relax }}{68}{figure.caption.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3-16}{\ignorespaces Data MC correction induced by $K_S^0$ classifier.\relax }}{69}{figure.caption.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.9}Summary}{69}{subsection.3.2.9}\protected@file@percent }
\@setckpt{chap3}{
\setcounter{page}{70}
\setcounter{equation}{4}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{2}
\setcounter{subsection}{9}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{16}
\setcounter{table}{3}
\setcounter{savepage}{3}
\setcounter{parentequation}{0}
\setcounter{float@type}{4}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{36}
\setcounter{r@tfl@t}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{2}
}
